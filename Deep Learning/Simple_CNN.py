# -*- coding: utf-8 -*-
"""20311142_Sum_Aaron_ex2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W4knh9C9X-KyPllsHVmGQqFkujD-3yqP

# MMAI 894 - Exercise 2
## Convolutional artificial neural network : Image classification
The goal of this excercise is to build a convolutional neural network using the tensorflow/keras library. We will be using the MNIST dataset.
Submission instructions:

- You cannot edit this notebook directly. Save a copy to your drive, and make sure to identify yourself in the title using name and student number
- Do not insert new cells before the final one (titled "Further exploration") 
- Verify that your notebook can _restart and run all_. 
- Select File -> Download as .py (important! not as ipynb)
- Rename the file: `studentID_lastname_firstname_ex2.py`
- The mark will be assessed on the implementation of the functions with #TODO
- **Do not change anything outside the functions**  unless in the further exploration section
- As you are encouraged to explore the network configuration, 20% of the mark is based on final accuracy. 
- Note: You do not have to answer the questions in thie notebook as part of your submission. They are meant to guide you.

- You should not need to use any additional libraries other than the ones listed below. You may want to import additional modules from those libraries, however.
"""

# Import modules
# Add modules as needed
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
import numpy as np

# For windows laptops add following 2 lines:
import matplotlib
matplotlib.use('agg')

import matplotlib.pyplot as plt

import tensorflow.keras as keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout,Conv2D,MaxPooling2D,Flatten

"""### Data preparation

#### Import data
"""

def load_data():
    # Import MNIST dataset from openml
    dataset = fetch_openml('mnist_784', version=1, data_home=None)

    # Data preparation
    raw_X = dataset['data']
    raw_Y = dataset['target']
    return raw_X, raw_Y

raw_X, raw_Y = load_data()

"""## Consider the following
- Same as excercise 1
- what shape should x be for a convolutional network?
"""

from tensorflow.python.keras.utils.np_utils import to_categorical
def clean_data(raw_X, raw_Y):
    # TODO: clean and QA raw_X and raw_Y
    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION
    cleaned_X = raw_X.astype('float32')
    cleaned_X /= 255
    cleaned_X=np.array(cleaned_X)
    cleaned_X = cleaned_X.reshape(cleaned_X.shape[0], 28, 28, 1)
    
    num_classes = 10
    cleaned_Y = keras.utils.to_categorical(raw_Y, num_classes)
    
        
    return cleaned_X, cleaned_Y

cleaned_X, cleaned_Y = clean_data(raw_X, raw_Y)

"""#### Data split

- Split your data into a train set (50%), validation set (20%) and a test set (30%). You can use scikit-learn's train_test_split function.
"""

def split_data(cleaned_X, cleaned_Y):
    # TODO: split the data
    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION
    X_train, X_test, Y_train, Y_test = train_test_split(cleaned_X, cleaned_Y, test_size=0.3, random_state=42)
    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=2/7, random_state=42)

    
    return X_val, X_test, X_train, Y_val, Y_test, Y_train

X_val, X_test, X_train, Y_val, Y_test, Y_train = split_data(cleaned_X, cleaned_Y)

"""### Model

#### Neural network structure

This time, the exact model architecture is left to you to explore.  
Keep the number of parameters below 2,000,000
"""

def build_model():
    # TODO: build the model, 
    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION
    model = Sequential()
    #1st hidden layer with 32 filter, 3x3 kernal size
    model.add(Conv2D(32,(3,3), activation ="relu",name='layer1',input_shape=(28,28,1)))

    #2nd hidden layer with 64 filters,3x3 kernal
    model.add(Conv2D(64,(3,3), activation='relu',name='layer2'))
    #halfed output by pooling
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Dropout(0.25))
    model.add(Flatten())

    #3rd hidden layer, Dense layer with 128 neurons
    model.add(Dense(128,activation='relu',name='layer3'))
    model.add(Dropout(0.5))
    #output layer
    model.add(Dense(10,activation='softmax'))
    
    
    return model

def compile_model(model):
    # TODO: compile the model
    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION
    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])



    return model

def train_model(model, X_train, Y_train, X_val, Y_val):
    # TODO: train the model
    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION



    history = model.fit(X_train, Y_train, batch_size = 128, epochs = 12, verbose=1, validation_data=(X_val, Y_val))
    return model, history


def eval_model(model, X_test, Y_test):
    # TODO: evaluate the model
    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION
    score = model.evaluate(X_test, Y_test, verbose=1)
    test_loss = score[0] 
    test_accuracy = score[1]
    print('Test Loss:', '%.4f' % test_loss)
    print('Test Accuracy:', '%.4f' % test_accuracy)




    return test_loss, test_accuracy

## You may use this space (and add additional cells for exploration)

model = build_model()
model = compile_model(model)
model, history = train_model(model, X_train, Y_train, X_val, Y_val)
test_loss, test_accuracy = eval_model(model, X_test, Y_test)